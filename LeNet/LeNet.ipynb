{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "data_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # get data root path\n",
    "aud_path = os.path.join(data_root, \"dataset\", \"bird\")  # flower data set path\n",
    "assert os.path.exists(aud_path), \"{} path does not exist.\".format(aud_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate metadata\"\"\"\n",
    "def get_meta(path):\n",
    "    metadate = []\n",
    "    label_list = os.listdir(path)\n",
    "    for i,label in enumerate(label_list):\n",
    "        label_path = os.path.join(path,label)\n",
    "        aud_list = os.listdir(label_path)\n",
    "        for j, aud in enumerate(aud_list):\n",
    "            aud_path = os.path.join(label_path,aud)\n",
    "            metadate.append([aud,aud_path,label,i])\n",
    "    df = pd.DataFrame(metadate,columns=['filename','path','label','label_id'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "aud_meta = get_meta(aud_path)\n",
    "aud_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre\n",
    "DURATION = 2000\n",
    "SR = 44100\n",
    "CHANNEL = 2\n",
    "N_MELS = 64\n",
    "N_FFT = 512\n",
    "F_MIN = 150\n",
    "F_MAX = 15_000\n",
    "HOP_LEN = 0.75 * N_FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Data\"\"\"\n",
    "def load_data(df):\n",
    "    def load_row(row):\n",
    "        audio_file = str(row.path)\n",
    "        \n",
    "        aud = torchaudio.load(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, SR)# 标准化采样\n",
    "        rechan = AudioUtil.rechannel(reaud, CHANNEL)#转换为 通道\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, DURATION)#统一为 秒\n",
    "        \n",
    "        sgram = AudioUtil.spectro_gram(dur_aud, n_mels=N_MELS, n_fft=N_FFT, f_min=F_MIN, f_max=F_MAX, hop_len=HOP_LEN)\n",
    "\n",
    "        return row.filename, sgram\n",
    "    pool = joblib.Parallel(4)\n",
    "    mapper = joblib.delayed(load_row)\n",
    "    tasks = [mapper(row) for row in df.itertuples(False)]\n",
    "    res = pool(tqdm(tasks))\n",
    "    res = dict(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read memory\n",
    "dataset = load_data(aud_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.meta = meta.copy().reset_index(drop=True)\n",
    "        self.sr = sr\n",
    "        self.is_train = is_train\n",
    "        self.num_classes = num_classes\n",
    "        self.duration = duration\n",
    "        self.audio_length = self.duration*self.sr\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(image):\n",
    "        image = image / 255.0\n",
    "#         image = torch.stack([image, image, image])\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def Augment(image):\n",
    "        # mask time and freq\n",
    "        image = spectro_augment(image, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta.iloc[idx]\n",
    "        image = self.dataset[row.filename]\n",
    "\n",
    "#         image = image[np.random.choice(len(image))]\n",
    "        \n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        t = row.label_id\n",
    "        return image, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BirdClefDataset(dataset, meta=df, sr=SR, duration=DURATION)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 0.8\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(ds)*SPLIT)  #这里train_size是一个长度矢量，并非是比例，我们将训练和测试进行8/2划分\n",
    "test_size = len(ds) - train_size\n",
    "\n",
    "train_dataset, test_dataset =random_split(ds, [train_size, test_size])\n",
    "train_dataset.is_train = True\n",
    "test_dataset.is_train = False\n",
    "print(train_dataset.is_train,test_dataset.is_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "![这是图片](./LeNet.png \"LeNet 结构\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(120,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,20)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))    # input(2, 32, 32) output(6, 28, 28)\n",
    "        x = self.pool1(x)            # output(6, 14, 14)\n",
    "        x = F.relu(self.conv2(x))    # output(16, 10, 10)\n",
    "        x = self.pool2(x)            # output(16, 5, 5)\n",
    "        x = x.view(-1, 32*5*5)       # output(16*5*5)\n",
    "        x = F.relu(self.fc1(x))      # output(120)\n",
    "        x = F.relu(self.fc2(x))      # output(84)\n",
    "        x = self.fc3(x)              # output(10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, test_dl, num_epochs):\n",
    "    # criterion, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                   steps_per_epoch=int(len(train_dl)),\n",
    "                                                   epochs = num_epochs,\n",
    "                                                   anneal_strategy='cos')\n",
    "    \n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    begin = datetime.datetime.now()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        train_loss = running_loss / num_batches\n",
    "        train_acc = correct_prediction/total_prediction\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_acc, test_loss = testing(model, test_dl)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        with open(\"1.txt\",'a') as f:\n",
    "            f.writelines(f'Epoch: {epoch}, Loss: {train_loss:.2f}, Accuracy: {train_acc:.2f}, \\\n",
    "            Test Accuracy: {test_acc:.2f}, Test loss: {test_loss:.2f}, Time: {datetime.datetime.now()-begin}\\n')\n",
    "            f.close()\n",
    "        print(f'Epoch: {epoch}, Loss: {train_loss:.2f}, Accuracy: {train_acc:.2f}',end=\" \")\n",
    "        print(f'Test Accuracy: {test_acc:.2f}, Test loss: {test_loss:.2f}, Time: {datetime.datetime.now()-begin}')\n",
    "        \n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_accs, train_losses, test_accs, test_losses\n",
    "\n",
    "def testing (model, test_dl):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "    \n",
    "    num_batches = len(test_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, train_losses, test_accs, test_losses= training(model,train_loader,test_loader,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_plot(train_accs, train_losses, test_accs, test_losses):\n",
    "    epochs = len(train_accs)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.linspace(1,epochs,epochs),train_accs,label=\"train_acc\")\n",
    "    plt.plot(np.linspace(1,epochs,epochs),test_accs,label=\"test_acc\")\n",
    "    plt.title(\"ACC Plot\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.linspace(1,epochs,epochs),train_losses,label=\"train_loss\")\n",
    "    plt.plot(np.linspace(1,epochs,epochs),test_losses,label=\"test_loss\")\n",
    "    plt.title(\"LOSS Plot\")\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"acc_loss.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_plot(train_accs, train_losses, test_accs, test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
